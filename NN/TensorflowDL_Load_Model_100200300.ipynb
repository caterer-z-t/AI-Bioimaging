{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e05890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi Layer Perceptron for Binary Classification\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea5a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 24  25  26  27  28  29  30  31  32  33  34  35  78  79  81  82  84  86\n",
      "   87  88  89  91  92  93 155 156 157 158 159 160 161 162 163 164 165 166\n",
      "  167 168 169 170 171 172 173 186 187 188 189 191 192 193 194 196 197 198\n",
      "  199 200 201 203 204 205 206 207 208 210 281 283 284 290 292 293 306 309\n",
      "  313 319 321 327 330 331 336 337 338 619 620 621 622 623 624 710 714 718\n",
      "  719 721 727 732 735 738 743 744 747 749 751 758 759 766]]\n",
      "[[164 165 166 189 191 192 193 196 199 200 203 204]]\n",
      "[[ 79  82  87  88  89 167 168 169 170 171 196 197 620 622 624 727]]\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "#Load the Datasets\n",
    "path = 'raw_mosaic_pixeldata_NN.csv'\n",
    "df = read_csv(path, header=None)\n",
    "\n",
    "feature_path = 'ETC_RFE_ANOVA.csv'\n",
    "feature_df = read_csv(feature_path, header=None)\n",
    "feature_array_triple = feature_df.to_numpy()\n",
    "print(feature_array_triple)\n",
    "\n",
    "feature_path = '12_features.csv'\n",
    "feature_df = read_csv(feature_path, header=None)\n",
    "feature_array_twelve = feature_df.to_numpy()\n",
    "print(feature_array_twelve)\n",
    "\n",
    "feature_path = 'LIT_Bands_Intersection_Feature_Selection.csv'\n",
    "feature_df = read_csv(feature_path, header=None)\n",
    "feature_array_LIT_intersect = feature_df.to_numpy()\n",
    "print(feature_array_LIT_intersect)\n",
    "\n",
    "feature_path = 'Literature_Bands_61.csv'\n",
    "feature_df = read_csv(feature_path, header=None)\n",
    "feature_array_LIT_61 = feature_df.to_numpy()\n",
    "print(feature_array_LIT_61.size)\n",
    "\n",
    "#Function used to return just the values in the list, can be used for 104 and 12 feature lists\n",
    "def array_values(list):\n",
    "    for i in list:\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca2ce251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25247, 767) (10821, 767) (25247,) (10821,)\n"
     ]
    }
   ],
   "source": [
    "#Split into input/output columns\n",
    "#Encode Strings to integers\n",
    "#Split into Training and Test Datasets\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n",
    "n_features = X_train.shape[1]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b15d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model first time\n",
    "\n",
    "#Define Model\n",
    "#767 Bands\n",
    "N = 100\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(N, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model_1.add(Dense(2*N, activation='relu', kernel_initializer='he_normal'))\n",
    "model_1.add(Dense(3*N, activation='relu', kernel_initializer='he_normal'))\n",
    "model_1.add(Dense(2*N, activation='relu', kernel_initializer='he_normal'))\n",
    "model_1.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Model\n",
    "model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC', 'Precision', 'Recall'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a1754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save the initial weights\n",
    "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
    "model_1.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##DO ALL YOUR TRAINING USING MODEL_1###\n",
    "start_time = time.time()\n",
    "history_1 = model_1.fit(X_train, y_train, epochs=100, batch_size=300, validation_split=0.3)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model\n",
    "model_1.save('767Bands_100200300_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, auc, precision, recall  = model_1.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: %.3f' % loss)\n",
    "print('Test Accuracy: %.3f' % accuracy)\n",
    "print('Test AUC: %.3f' % auc)\n",
    "print('Test Precision: %.3f' % precision)\n",
    "print('Test Recall: %.3f' % recall)\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "\n",
    "y_pred = model_1.predict(X_test)\n",
    "y_pred = np.round(y_pred).tolist()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "hist_df = pd.DataFrame(history_1.history)\n",
    "cm_df = pd.DataFrame(cm)\n",
    "\n",
    "output_csv_file = '767Bands_100200300_Output.csv'\n",
    "with open(output_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "    f.write('\\n')\n",
    "    cm_df.to_csv(f, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##NOW ITS TIME TO RELOAD THE OLD WEIGHTS IN MODEL_1#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2477b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where the old weights without training are being loaded\n",
    "model_1.load_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()\n",
    "#The summary is same as model_1\n",
    "#Our next goal is to modify the input shape. \n",
    "#For doing this we will \n",
    "#Then we will add a new layer which matching input dimensions that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2 -- 3 Feature Selection Algorithms\n",
    "X, y = df.values[:, array_values(feature_array_triple)-1], df.values[:, -1]\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95dcfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_1 has the loaded weights. \n",
    "#We are generating a new model_2 here.\n",
    "#This is our final new model where we will copy the preliminary weights from model_1\n",
    "model_2 = Sequential()\n",
    "#create a new first layer with desired input_dimensions\n",
    "model_2.add(Dense(N, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "#add layers with loaded weights sequentially \n",
    "# go through all layers but the first one\n",
    "for layer in model_1.layers[1:]: \n",
    "    model_2.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC', 'Precision', 'Recall'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=100, batch_size=300, validation_split=0.3)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cfa343",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save('3Feature_100200300_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, auc, precision, recall  = model_2.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: %.3f' % loss)\n",
    "print('Test Accuracy: %.3f' % accuracy)\n",
    "print('Test AUC: %.3f' % auc)\n",
    "print('Test Precision: %.3f' % precision)\n",
    "print('Test Recall: %.3f' % recall)\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "\n",
    "y_pred = model_2.predict(X_test)\n",
    "y_pred = np.round(y_pred).tolist()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "hist_df = pd.DataFrame(history_2.history)\n",
    "cm_df = pd.DataFrame(cm)\n",
    "\n",
    "output_csv_file = '3Feature_100200300_Output.csv'\n",
    "with open(output_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "    f.write('\\n')\n",
    "    cm_df.to_csv(f, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.load_weights(initial_weights)\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3 -- Literature Band Intersection\n",
    "X, y = df.values[:, array_values(feature_array_LIT_intersect)-1], df.values[:, -1]\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_1 has the loaded weights. \n",
    "#We are generating a new model_3 here.\n",
    "#This is our final new model where we will copy the preliminary weights from model_1\n",
    "model_3 = Sequential()\n",
    "#create a new first layer with desired input_dimensions\n",
    "model_3.add(Dense(N, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "#add layers with loaded weights sequentially \n",
    "# go through all layers but the first one\n",
    "for layer in model_1.layers[1:]: \n",
    "    model_3.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC', 'Precision', 'Recall'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ced38",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "history_3 = model_3.fit(X_train, y_train, epochs=100, batch_size=300, validation_split=0.3)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.save('LitIntersection_100200300_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, auc, precision, recall  = model_3.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: %.3f' % loss)\n",
    "print('Test Accuracy: %.3f' % accuracy)\n",
    "print('Test AUC: %.3f' % auc)\n",
    "print('Test Precision: %.3f' % precision)\n",
    "print('Test Recall: %.3f' % recall)\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "\n",
    "y_pred = model_3.predict(X_test)\n",
    "y_pred = np.round(y_pred).tolist()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "hist_df = pd.DataFrame(history_3.history)\n",
    "cm_df = pd.DataFrame(cm)\n",
    "\n",
    "output_csv_file = 'LitIntersection_100200300_Output.csv'\n",
    "with open(output_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "    f.write('\\n')\n",
    "    cm_df.to_csv(f, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1af835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.load_weights(initial_weights)\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1084f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 4 -- Literature Bands 61\n",
    "X, y = df.values[:, array_values(feature_array_LIT_61)-1], df.values[:, -1]\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dacbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_1 has the loaded weights. \n",
    "#We are generating a new model_4 here.\n",
    "#This is our final new model where we will copy the preliminary weights from model_1\n",
    "model_4 = Sequential()\n",
    "#create a new first layer with desired input_dimensions\n",
    "model_4.add(Dense(N, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "#add layers with loaded weights sequentially \n",
    "# go through all layers but the first one\n",
    "for layer in model_1.layers[1:]: \n",
    "    model_4.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bc27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC', 'Precision', 'Recall'])\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5958bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "history_4 = model_4.fit(X_train, y_train, epochs=100, batch_size=300, validation_split=0.3)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff746ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.save('LitBands61_100200300_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, auc, precision, recall  = model_4.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: %.3f' % loss)\n",
    "print('Test Accuracy: %.3f' % accuracy)\n",
    "print('Test AUC: %.3f' % auc)\n",
    "print('Test Precision: %.3f' % precision)\n",
    "print('Test Recall: %.3f' % recall)\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "\n",
    "y_pred = model_4.predict(X_test)\n",
    "y_pred = np.round(y_pred).tolist()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "hist_df = pd.DataFrame(history_4.history)\n",
    "cm_df = pd.DataFrame(cm)\n",
    "\n",
    "output_csv_file = 'LitBands61_100200300_Output.csv'\n",
    "with open(output_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "    f.write('\\n')\n",
    "    cm_df.to_csv(f, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
